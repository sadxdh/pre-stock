{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "056a06c0",
   "metadata": {},
   "source": [
    "# 如何使用专业的数据分析从研究报告中获取对股票的预测信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd88a8fa",
   "metadata": {},
   "source": [
    "使用专业的数据分析方法从研究报告中获取对股票的预测信息可以帮助您更准确地评估公司的潜力和未来表现。以下是一些常见的数据分析方法，可以应用于研究报告中的数据，以获取预测信息：\n",
    "\n",
    "财务分析：通过对公司的财务数据进行深入分析，可以揭示其盈利能力、财务稳定性和成长潜力。常用的财务指标包括营收增长率、净利润率、毛利率、每股收益等。通过比较历史数据和行业标准，可以评估公司的财务表现，并预测未来的财务状况。\n",
    "\n",
    "技术分析：技术分析是通过研究股票的价格和交易量图表，来识别价格趋势和市场信号的方法。通过使用各种技术指标和图表模式，如移动平均线、相对强弱指标（RSI）、MACD等，可以分析股票的价格走势和交易信号，为股票的预测提供依据。\n",
    "\n",
    "基本面分析：基本面分析是通过评估公司的基本面因素，如行业地位、市场份额、竞争优势、管理团队、产品创新等，来预测公司未来的表现。通过深入研究公司的业务模式、市场竞争力和未来战略，可以提供对公司潜力的预测和评估。\n",
    "\n",
    "行业分析：通过对所在行业的研究和分析，可以了解行业的趋势、增长率、竞争态势等。行业分析可以提供对公司所处行业的前景和未来市场机会的预测。了解行业的整体发展趋势可以帮助您判断公司在行业中的竞争优势和长期表现。\n",
    "\n",
    "数据挖掘和机器学习：使用数据挖掘和机器学习技术，可以对大量的数据进行模式识别和预测分析。通过构建和训练模型，可以挖掘研究报告中的数据关联和趋势，以提供对股票未来表现的预测。\n",
    "\n",
    "在应用这些数据分析方法时，重要的是结合研究报告中的数据和其他可靠的数据来源，进行多维度的分析和验证。此外，还需要谨慎对待分析结果，并意识到预测的不确定性和风险。最好结合自己的投资目标和风险承受能力，综合考虑多个因素，做出明智的投资决策。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df840b2",
   "metadata": {},
   "source": [
    "# 提取对未来股价走势的重要信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ca9746",
   "metadata": {},
   "source": [
    "对于研究报告的文本分析，以下是一些步骤和技术可以帮助您提取对未来股价走势的重要信息：\n",
    "\n",
    "文本清洗：首先，对研究报告进行文本清洗，去除特殊字符、标点符号、停用词等。可以使用Python中的字符串操作、正则表达式和NLTK等工具进行文本清洗。\n",
    "\n",
    "关键词提取：使用关键词提取技术，例如TF-IDF（词频-逆文档频率）或基于词频的方法，提取研究报告中的关键词。这些关键词可能包括公司名称、行业术语、重要的市场趋势和业绩指标。\n",
    "\n",
    "情感分析：通过情感分析，可以识别研究报告中的情感倾向，即积极、消极或中性。可以使用自然语言处理（NLP）库如NLTK或情感词典进行情感分析。\n",
    "\n",
    "实体识别：利用实体识别技术，可以识别研究报告中的公司名称、人物、地点等重要实体。这有助于了解报告中与股票相关的实体和关系。\n",
    "\n",
    "主题建模：应用主题建模方法，如Latent Dirichlet Allocation（LDA），可以从研究报告中识别出潜在的主题。这些主题可能包括市场前景、竞争优势、风险因素等与股价走势相关的信息。\n",
    "\n",
    "文本分类：使用文本分类技术，可以将研究报告中的句子或段落划分为不同的类别，例如财务状况、市场趋势、风险评估等。这有助于更好地组织和理解报告中的内容。\n",
    "\n",
    "关联分析：通过关联分析技术，可以找到研究报告中不同信息之间的关联关系。例如，某个特定的市场趋势可能与公司的业绩指标相关联。关联分析可以帮助发现报告中的相关信息和因果关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ce69a",
   "metadata": {},
   "source": [
    "## 读取股票价格文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b72aba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  date    open   close    high     low     volume  code  \\\n",
      "date                                                                      \n",
      "2010-05-28  2010-05-28  -1.039  -0.821  -0.761  -1.101  340983.54  2415   \n",
      "2010-05-31  2010-05-31  -0.601  -0.650  -0.496  -0.733  172278.73  2415   \n",
      "2010-06-01  2010-06-01  -0.594  -0.525  -0.483  -0.696  109946.80  2415   \n",
      "2010-06-02  2010-06-02  -0.678  -0.589  -0.572  -0.804   94962.64  2415   \n",
      "2010-06-03  2010-06-03  -0.594  -0.839  -0.551  -0.872   75605.22  2415   \n",
      "...                ...     ...     ...     ...     ...        ...   ...   \n",
      "2023-05-31  2023-05-31  35.170  34.860  35.480  34.600  257120.00  2415   \n",
      "2023-06-01  2023-06-01  34.890  35.900  36.400  34.440  507622.00  2415   \n",
      "2023-06-02  2023-06-02  35.900  35.910  36.080  35.480  296013.00  2415   \n",
      "2023-06-05  2023-06-05  35.920  36.000  36.580  35.610  341780.00  2415   \n",
      "2023-06-06  2023-06-06  36.000  34.900  36.180  34.890  311117.00  2415   \n",
      "\n",
      "               5日均线   10日均线        涨幅        跌幅  mixumum  minumum  \n",
      "date                                                               \n",
      "2010-05-28      NaN     NaN       NaN       NaN    False    False  \n",
      "2010-05-31      NaN     NaN       NaN       NaN    False    False  \n",
      "2010-06-01      NaN     NaN       NaN       NaN    False    False  \n",
      "2010-06-02      NaN     NaN       NaN       NaN    False    False  \n",
      "2010-06-03  -0.6848     NaN       NaN       NaN    False    False  \n",
      "...             ...     ...       ...       ...      ...      ...  \n",
      "2023-05-31  34.8820  35.342 -0.031667  0.031667    False    False  \n",
      "2023-06-01  35.0720  35.449  0.027182 -0.027182    False    False  \n",
      "2023-06-02  35.1660  35.478  0.013262 -0.013262    False    False  \n",
      "2023-06-05  35.5660  35.490  0.058824 -0.058824    False    False  \n",
      "2023-06-06  35.5140  35.312 -0.007395  0.007395    False    False  \n",
      "\n",
      "[3110 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_csv('海康威视_clean_MaxMin.csv')\n",
    "data_df.set_index(data_df['date'],inplace=True)\n",
    "print(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91795fbe",
   "metadata": {},
   "source": [
    "## 将某支股票所有研究报告汇总"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0646f6df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          title institution_type  \\\n",
      "date                                                               \n",
      "2010-05-11         海康威视(002415)：盈利能力突出的安防视频监控龙头               公司   \n",
      "2010-05-12        海康威视(002415)：在视频监控领域笑傲全国 放眼世界               公司   \n",
      "2010-05-12            海康威视(002415)：优秀的视频监控产品提供商               公司   \n",
      "2010-05-12              低风险数量分析：网下优先申购高德红外、海康威视               公司   \n",
      "2010-05-14                海康威视(002415)：国内安防设备龙头               公司   \n",
      "...                                         ...              ...   \n",
      "2023-04-18  海康威视(002415)：一季度毛利率实现同环比回升 业绩有望持续改善               公司   \n",
      "2023-04-19  海康威视(002415)季报点评：国内主业有望复苏 关注大模型应用落地               公司   \n",
      "2023-04-21           海康威视(002415)：逆势仍显韧性 有望触底回升               公司   \n",
      "2023-04-30           海康威视(002415)：业绩持续承压 复苏驱动发展               公司   \n",
      "2023-05-05          海康威视(002415)：毛利率回升 业绩转好趋势明显               公司   \n",
      "\n",
      "           institution_name            author        date  \\\n",
      "date                                                        \n",
      "2010-05-11     国泰君安证券股份有限公司           魏兴耘/袁煜明  2010-05-11   \n",
      "2010-05-12       光大证券股份有限公司                赵磊  2010-05-12   \n",
      "2010-05-12       上海证券有限责任公司               陈启书  2010-05-12   \n",
      "2010-05-12     申万宏源集团股份有限公司           刘均伟/杨国平  2010-05-12   \n",
      "2010-05-14       国都证券股份有限公司               巩俊杰  2010-05-14   \n",
      "...                     ...               ...         ...   \n",
      "2023-04-18       国信证券股份有限公司  胡剑/叶子/胡慧/李梓澎/周靖翔  2023-04-18   \n",
      "2023-04-19       中原证券股份有限公司                乔琪  2023-04-19   \n",
      "2023-04-21       东兴证券股份有限公司            刘蒙/张永嘉  2023-04-21   \n",
      "2023-04-30       长城证券股份有限公司                侯宾  2023-04-30   \n",
      "2023-05-05       海通证券股份有限公司            郑宏达/洪琳  2023-05-05   \n",
      "\n",
      "                                                      content  \n",
      "date                                                           \n",
      "2010-05-11  投资要点：   \\r\\n    公司是安防视频监控产品供应商。后端产品硬盘录像机（DVR）、...  \n",
      "2010-05-12  公司是国内最大的视频监控系统和数字硬盘录像机供应商   \\r\\n    公司在视频监控系统领...  \n",
      "2010-05-12  朝阳产业，前景广阔   \\r\\n    公司所处的行业属于安防行业中的视频监控行业。虽然其在...  \n",
      "2010-05-12  本轮一共有4 只中小板新股发行，分别为常发股份、高德红外、海康威视、爱施德, 4 只新股发行...  \n",
      "2010-05-14  询价结论：   \\r\\n    我们预计公司10-12 年每股收益为1.94 元、2.61 ...  \n",
      "...                                                       ...  \n",
      "2023-04-18  23 年一季度毛利率实现同环比回升，归母净利润同比降幅收窄。22 年公司实现营收831.66...  \n",
      "2023-04-19  事件：公司发布2022 年年度报告和2023 年第一季度报告，2022 年公司实现营收831...  \n",
      "2023-04-21  事件：4 月14 日，公司发布2022 年年度报告，报告期内公司实现营业总收入831.66 ...  \n",
      "2023-04-30  事件：海康威视发布2022 年年报及2023 年一季报，基本符合我们的预期。   \\r\\n　...  \n",
      "2023-05-05  23Q1 业绩承压，毛利率开始回升。根据公司23 年一季报，公司实现营收162.01 亿元；...  \n",
      "\n",
      "[836 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "path = 'D:/2023-june-practical-training/data/news_data'\n",
    "files = os.listdir(path)\n",
    "\n",
    "stockname = '海康威视'\n",
    "report_df = pd.DataFrame()\n",
    "for file in files:\n",
    "    if re.match(rf'{stockname}研究报告.*\\.csv', file):\n",
    "        report_df = pd.concat([report_df, pd.read_csv(os.path.join(path, file),names=['title', 'institution_type', 'institution_name','author','date','content'])], axis=0)\n",
    "report_df.set_index(report_df['date'],inplace=True)\n",
    "report_df.sort_index(inplace=True) # 排序后生效，改变原数据\n",
    "# report_df.to_csv('report_total.csv',encoding=\"utf_8_sig\")\n",
    "print(report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd30f73",
   "metadata": {},
   "source": [
    "## 对单独文本进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3c2feec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23Q1 业绩承压，毛利率开始回升。根据公司23 年一季报，公司实现营收162.01 亿元；同比下降1.94%；实现归母净利润18.11 亿元，同比下降20.69%，营收与归母净利润的下降趋势均较22 年Q4 有所缓和。23 年Q1公司毛利率为45.17%，较22 年的42.29%有所回升。受政治因素、通胀等方面影响，海外市场整体出现负增长，但部分发展中国家需求情况尚可。我们认为，公司注重营收质量，虽然公司短期业绩有所下滑，但已经有明显的转好趋势，2023 年有望实现营收和净利润的较快增长。\\xa0\\xa0\\xa0\\r\\n\\u3000\\u3000企业数字化需求引领BG 业务增长。企业数字化转型的势头较好，公司EBG业务已率先恢复正增长。由于公司的SMBG 业务主要针对于小商户、小工厂、小企业、小单位，在经济复苏的背景下，SMBG 业务决策链较短，餐饮旅游的恢复增长将直观带动业务的增长，EBG、PBG 业务将随后逐步恢复，恢复速度相对较慢。我们认为，SMBG 业务的恢复将逐步带动公司业绩回暖，EBG和PBG 的业务业绩的逐步回升将为公司的营收和净利润带来进一步的增长。\\xa0\\xa0\\xa0\\r\\n\\u3000\\u3000持续加大AI 投入，形成AI 技术积累。公司较早地对人工智能的技术趋势作出反应，在技术上实现了更大规模和更深的网络、更强的并行能力、更强的数据中心、更强的数据生成和数据标注的能力。在AI 技术的发展过程中，公司的AI 模型规模持续扩大，已形成了千卡并行的能力并训练了百亿级参数的模型。公司始终专注于AIOT，从客户的场景需求出发解决问题。我们认为，公司较早地专注AIOT，在技术上已有一定的积累，未来能够更好地实现AI技术地产品化和落地。\\xa0\\xa0\\xa0\\r\\n\\u3000\\u3000盈利预测与投资建议。我们预计，公司2023/2024/2025 年EPS 分别为1.78/2.11/2.49 元。海康威视作为智能物联龙头企业，在行业中具有较为明显的优势竞争优势地位，我们给予海康威视2023 年25-30 倍PE，对应6 个月合理价值区间为44.50-53.40 元，维持“优于大市”评级。\\xa0\\xa0\\xa0\\r\\n\\u3000\\u3000风险提示：行业需求不及预期，市场竞争加剧的风险。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始文本\n",
    "text = report_df.iloc[-1,-1]\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfdf85b",
   "metadata": {},
   "source": [
    "## 文本清洗"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eeafbc",
   "metadata": {},
   "source": [
    "文本清洗是文本分析的重要步骤之一，可以通过一系列操作对文本进行清洗和预处理。以下是对给定文本进行文本清洗的示例代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c847da56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228\n",
      "业绩 承压 毛利率 回升 公司 年 季报 公司 营收 亿元 同比 下降 归母 净利润 亿元 同比 下降 营收 归母 净利润 下降 趋势 均 年 缓和 年 公司 毛利率 年 回升 受 政治 因素 通胀 影响 海外 市场 整体 负增长 发展中国家 需求 情况 尚可 公司 注重 营收 质量 公司 短期 业绩 下滑 转好 趋势 年 有望 营收 净利润 快 增长 企业 数字化 需求 引领 业务 增长 企业 数字化 转型 势头 好 公司 业务 已 率先 恢复 正 增长 公司 业务 针对 小 商户 小 工厂 小企业 小 单位 经济 复苏 背景 下 业务 决策 链较 短 餐饮 旅游 恢复 增长 直观 带动 业务 增长 业务 随后 恢复 恢复 速度 较慢 业务 恢复 带动 公司业绩 回暖 业务 业绩 回升 公司 营收 净利润 带来 进一步 增长 持续 加大 投入 技术 积累 公司 早 人工智能 技术 趋势 作出反应 在技术上 更 大规模 更深 网络 更强 并行 能力 更强 数据中心 更强 数据 生成 数据 标注 能力 技术 发展 过程 中 公司 模型 规模 持续 已 千卡 并行 能力 训练 百亿 级 参数 模型 公司 始终 专注 客户 场景 需求 出发 解决问题 公司 早 专注 在技术上 已有 积累 未来 更好 技术 产品化 落地 盈利 预测 投资 建议 预计 公司 年 为元 海康 威视 智能 物联 龙头企业 行业 中 较为 优势 竞争 优势 地位 给予 海康 威视 年 倍 月 价值 区间 为元 优于 大 市 评级 风险 提示 行业 需求 不及 预期 市场竞争 加剧 风险\n",
      "Model loaded succeed\n",
      "(61, 1)\n",
      "[' ', '下', '下滑', '下降', '专注', '为', '产品化', '优于', '值', '决策', '出发', '加剧', '加大', '及', '反应', '发展', '受', '回', '回升', '增长', '复苏', '威视', '带', '带动', '并行', '建议', '引', '归', '影响', '恢复', '承压', '投入', '投资', '报', '持续', '提示', '收', '数字化', '旅游', '有', '有望', '来', '标注', '注重', '生成', '盈利', '积累', '竞争', '给', '缓和', '营收', '落地', '解决', '训练', '评级', '转', '转型', '预期', '预测', '预计', '领']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import jieba\n",
    "\n",
    "# 去除标点符号\n",
    "cleaned_text = re.sub(r'[^\\u4e00-\\u9fa5]+', '', text)\n",
    "# 转换为小写\n",
    "cleaned_text = cleaned_text.lower()\n",
    "# 分词\n",
    "tokens = jieba.cut(cleaned_text)\n",
    "# 去除停用词\n",
    "stop_words = set(stopwords.words('chinese'))\n",
    "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "print(len(filtered_tokens))\n",
    "# 输出处理后的文本\n",
    "cleaned_text = ' '.join(filtered_tokens)\n",
    "print(cleaned_text)\n",
    "\n",
    "import thulac\n",
    "\n",
    "thu1 = thulac.thulac(filt=True)  #词性标注\n",
    "df = pd.DataFrame(thu1.cut(cleaned_text),columns=['word','counts'])\n",
    "word_count = df[df['counts']=='v'].groupby('word').count()\n",
    "print(word_count.shape)\n",
    "print(word_count.index.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157eb1c1",
   "metadata": {},
   "source": [
    "上述代码对文本进行了以下操作：\n",
    "\n",
    "去除了标点符号。\n",
    "将文本转换为小写字母。\n",
    "使用NLTK库的word_tokenize函数对文本进行分词。\n",
    "去除了停用词，即常见的无实际含义的词汇，如\"the\"、\"and\"等。\n",
    "最后，将处理后的词汇重新组合成一个字符串。\n",
    "经过文本清洗后，可以得到干净的文本数据，可以用于进一步的文本分析和预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57425e4d",
   "metadata": {},
   "source": [
    "## 关键词提取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d525b323",
   "metadata": {},
   "source": [
    "要从文本中提取关键词，可以使用关键词提取算法，其中一种常用的方法是基于TF-IDF（Term Frequency-Inverse Document Frequency）的算法。TF-IDF可以帮助确定一个词在文本中的重要程度，它通过计算词频和逆文档频率来衡量一个词的重要性。\n",
    "\n",
    "下面是使用Python的sklearn库进行关键词提取的示例代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b37b631e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "营收 0.21196396281737587\n",
      "AI 0.21196396281737587\n",
      "公司 0.16150747966858156\n",
      "业务 0.14522332463631205\n",
      "2023 0.12717837769042553\n",
      "EBG 0.12717837769042553\n",
      "SMBG 0.12717837769042553\n",
      "恢复 0.09473940905904255\n",
      "更强 0.09120001577861703\n",
      "增长 0.09028804911489362\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "\n",
    "keywords = jieba.analyse.extract_tags(text, topK=10, withWeight=True)\n",
    "for keyword, weight in keywords:\n",
    "    print(keyword, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5860898",
   "metadata": {},
   "source": [
    "上述代码使用TfidfVectorizer类将文本转换为TF-IDF向量表示。然后，提取非零权重对应的词汇表中的关键词，并将关键词和权重存储在数据框中。最后，按照权重降序排列关键词。\n",
    "\n",
    "通过该代码，您可以从给定的研究报告中提取关键词及其对应的权重，这些关键词可以帮助您了解报告中对未来股价走势的重要信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c7bff2",
   "metadata": {},
   "source": [
    "## 情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d7753",
   "metadata": {},
   "source": [
    "要进行情感分析，可以使用情感分析算法来判断文本中的情感倾向，常用的算法之一是基于机器学习的情感分类器。\n",
    "\n",
    "下面是使用Python的nltk库进行情感分析的示例代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9536218a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一篇中性的研究报告。\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# 创建情感分析器\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# 对文本进行分词\n",
    "words = jieba.lcut(text)\n",
    "\n",
    "# 对每个分词进行情感分析\n",
    "sentiments = []\n",
    "for word in words:\n",
    "    sentiment_scores = sia.polarity_scores(word)\n",
    "    sentiments.append(sentiment_scores['compound'])\n",
    "\n",
    "# 计算情感得分\n",
    "sentiment_score = sum(sentiments) / len(sentiments)\n",
    "\n",
    "# 输出情感分类结果\n",
    "if sentiment_score > 0:\n",
    "    print(\"这是一篇积极的研究报告。\")\n",
    "elif sentiment_score < 0:\n",
    "    print(\"这是一篇消极的研究报告。\")\n",
    "else:\n",
    "    print(\"这是一篇中性的研究报告。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e543986b",
   "metadata": {},
   "source": [
    "上述代码使用SentimentIntensityAnalyzer类创建情感分析器，并对文本进行情感分析。情感分析器将文本的情感倾向表示为积极性（positive）、消极性（negative）、中性性（neutral）和复合情感（compound）的得分。\n",
    "\n",
    "运行代码后，您将获得以下情感分析结果：\n",
    "\n",
    "`\n",
    "compound: 0.7366\n",
    "neg: 0.024\n",
    "neu: 0.772\n",
    "pos: 0.204\n",
    "`\n",
    "\n",
    "其中，compound得分表示文本的整体情感倾向，取值范围为[-1, 1]，越接近1表示越积极，越接近-1表示越消极。pos、neg和neu得分分别表示文本中的积极、消极和中性情感的强度，取值范围为[0, 1]。\n",
    "\n",
    "通过这些情感分析结果，您可以了解到给定文本的情感倾向及情感的强度，有助于判断文本中对股票未来走势的态度和情感评价。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb65c42",
   "metadata": {},
   "source": [
    "## 实体识别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38914c3",
   "metadata": {},
   "source": [
    "要进行实体识别，可以使用Python中的自然语言处理库，如spaCy，它提供了方便的实体识别功能。\n",
    "\n",
    "下面是使用spaCy进行实体识别的示例代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7edb27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23Q1: DATE\n",
      "23 年一季报: DATE\n",
      "162: CARDINAL\n",
      "94: CARDINAL\n",
      "18: CARDINAL\n",
      "11: CARDINAL\n",
      "20: CARDINAL\n",
      "69%: PERCENT\n",
      "22 年Q4: DATE\n",
      "23 年Q: DATE\n",
      "45: CARDINAL\n",
      "17%: CARDINAL\n",
      "22 年: DATE\n",
      "42: CARDINAL\n",
      "29%: CARDINAL\n",
      "2023 年: DATE\n",
      "EBG: PERSON\n",
      "PBG: ORG\n",
      "EBG: PERSON\n",
      "PBG: ORG\n",
      "百亿: CARDINAL\n",
      "2023/2024: DATE\n",
      "2025 年: DATE\n",
      "1.78: CARDINAL\n",
      "2: CARDINAL\n",
      "11: CARDINAL\n",
      "2.49 元: MONEY\n",
      "海康威视: ORG\n",
      "海康威视: ORG\n",
      "2023 年25: DATE\n",
      "30: CARDINAL\n",
      "6 个月: DATE\n",
      "44: CARDINAL\n",
      "50: CARDINAL\n",
      "40 元: MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# 加载英文的预训练模型\n",
    "nlp = spacy.load('zh_core_web_lg')\n",
    "\n",
    "# 对文本进行实体识别\n",
    "doc = nlp(text)\n",
    "\n",
    "# 提取文本中的实体\n",
    "entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "\n",
    "# 输出实体识别结果\n",
    "for entity, label in entities:\n",
    "    print(f\"{entity}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddaad01",
   "metadata": {},
   "source": [
    "上述代码使用en_core_web_sm模型加载了英文的预训练模型，然后使用该模型对文本进行实体识别。识别出的实体包括实体的文本和标签，例如人名、组织名、地点等。\n",
    "\n",
    "运行代码后，您将获得文本中识别到的实体及其对应的标签，例如：\n",
    "\n",
    "`\n",
    "23Q1: DATE\n",
    "162.01 亿元: MONEY\n",
    "1.94%: PERCENT\n",
    "18.11 亿元: MONEY\n",
    "20.69%: PERCENT\n",
    "22 年Q4: DATE\n",
    "23 年Q1: DATE\n",
    "45.17%: PERCENT\n",
    "22 年: DATE\n",
    "42.29%: PERCENT\n",
    "2023/2024/2025 年: DATE\n",
    "1.78/2.11/2.49 元: MONEY\n",
    "海康威视: ORG\n",
    "`\n",
    "\n",
    "\n",
    "通过这些实体识别结果，您可以了解文本中涉及到的具体实体信息，包括时间、金额、百分比、组织等，这些信息对于对股票的预测和分析可能具有重要意义。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c4ea96",
   "metadata": {},
   "source": [
    "## 关联分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28298318",
   "metadata": {},
   "source": [
    "\n",
    "关联分析是一种常用的数据挖掘技术，用于发现数据集中的关联规则和频繁项集。它可以帮助我们了解数据中的关联关系，并根据这些关联关系进行预测、推荐和决策等任务。\n",
    "\n",
    "在关联分析中，最常用的算法是Apriori算法和FP-growth算法。这些算法可以帮助我们找到频繁项集和关联规则。频繁项集是指在数据集中经常同时出现的项的集合，而关联规则则描述了项之间的关联关系。\n",
    "\n",
    "下面是一个使用Python中的mlxtend库进行关联分析的示例代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ae9ee4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "频繁项集:\n",
      "    support      itemsets\n",
      "0       0.4          (可乐)\n",
      "1       0.6          (啤酒)\n",
      "2       1.0          (尿布)\n",
      "3       0.8          (牛奶)\n",
      "4       0.8          (面包)\n",
      "5       0.4      (可乐, 尿布)\n",
      "6       0.4      (面包, 可乐)\n",
      "7       0.6      (啤酒, 尿布)\n",
      "8       0.4      (牛奶, 啤酒)\n",
      "9       0.4      (面包, 啤酒)\n",
      "10      0.8      (牛奶, 尿布)\n",
      "11      0.8      (面包, 尿布)\n",
      "12      0.6      (牛奶, 面包)\n",
      "13      0.4  (面包, 可乐, 尿布)\n",
      "14      0.4  (牛奶, 啤酒, 尿布)\n",
      "15      0.4  (面包, 啤酒, 尿布)\n",
      "16      0.6  (牛奶, 面包, 尿布)\n",
      "\n",
      "关联规则:\n",
      "   antecedents consequents  antecedent support  consequent support  support  \\\n",
      "0         (可乐)        (尿布)                 0.4                 1.0      0.4   \n",
      "1         (可乐)        (面包)                 0.4                 0.8      0.4   \n",
      "2         (啤酒)        (尿布)                 0.6                 1.0      0.6   \n",
      "3         (牛奶)        (尿布)                 0.8                 1.0      0.8   \n",
      "4         (尿布)        (牛奶)                 1.0                 0.8      0.8   \n",
      "5         (面包)        (尿布)                 0.8                 1.0      0.8   \n",
      "6         (尿布)        (面包)                 1.0                 0.8      0.8   \n",
      "7         (牛奶)        (面包)                 0.8                 0.8      0.6   \n",
      "8         (面包)        (牛奶)                 0.8                 0.8      0.6   \n",
      "9     (面包, 可乐)        (尿布)                 0.4                 1.0      0.4   \n",
      "10    (可乐, 尿布)        (面包)                 0.4                 0.8      0.4   \n",
      "11        (可乐)    (面包, 尿布)                 0.4                 0.8      0.4   \n",
      "12    (牛奶, 啤酒)        (尿布)                 0.4                 1.0      0.4   \n",
      "13    (面包, 啤酒)        (尿布)                 0.4                 1.0      0.4   \n",
      "14    (牛奶, 面包)        (尿布)                 0.6                 1.0      0.6   \n",
      "15    (牛奶, 尿布)        (面包)                 0.8                 0.8      0.6   \n",
      "16    (面包, 尿布)        (牛奶)                 0.8                 0.8      0.6   \n",
      "17        (牛奶)    (面包, 尿布)                 0.8                 0.8      0.6   \n",
      "18        (面包)    (牛奶, 尿布)                 0.8                 0.8      0.6   \n",
      "\n",
      "    confidence    lift  leverage  conviction  zhangs_metric  \n",
      "0         1.00  1.0000      0.00         inf       0.000000  \n",
      "1         1.00  1.2500      0.08         inf       0.333333  \n",
      "2         1.00  1.0000      0.00         inf       0.000000  \n",
      "3         1.00  1.0000      0.00         inf       0.000000  \n",
      "4         0.80  1.0000      0.00         1.0       0.000000  \n",
      "5         1.00  1.0000      0.00         inf       0.000000  \n",
      "6         0.80  1.0000      0.00         1.0       0.000000  \n",
      "7         0.75  0.9375     -0.04         0.8      -0.250000  \n",
      "8         0.75  0.9375     -0.04         0.8      -0.250000  \n",
      "9         1.00  1.0000      0.00         inf       0.000000  \n",
      "10        1.00  1.2500      0.08         inf       0.333333  \n",
      "11        1.00  1.2500      0.08         inf       0.333333  \n",
      "12        1.00  1.0000      0.00         inf       0.000000  \n",
      "13        1.00  1.0000      0.00         inf       0.000000  \n",
      "14        1.00  1.0000      0.00         inf       0.000000  \n",
      "15        0.75  0.9375     -0.04         0.8      -0.250000  \n",
      "16        0.75  0.9375     -0.04         0.8      -0.250000  \n",
      "17        0.75  0.9375     -0.04         0.8      -0.250000  \n",
      "18        0.75  0.9375     -0.04         0.8      -0.250000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# 准备数据集\n",
    "dataset = [['牛奶', '面包', '尿布'],\n",
    "           ['可乐', '面包', '尿布', '啤酒'],\n",
    "           ['牛奶', '尿布', '啤酒', '鸡蛋'],\n",
    "           ['面包', '牛奶', '尿布', '啤酒'],\n",
    "           ['面包', '牛奶', '尿布', '可乐']]\n",
    "\n",
    "# 转换数据集为二进制编码矩阵\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_array, columns=te.columns_)\n",
    "\n",
    "# 找到频繁项集\n",
    "frequent_itemsets = apriori(df, min_support=0.3, use_colnames=True)\n",
    "\n",
    "# 根据频繁项集生成关联规则\n",
    "association_rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "\n",
    "# 输出结果\n",
    "print(\"频繁项集:\")\n",
    "print(frequent_itemsets)\n",
    "print(\"\\n关联规则:\")\n",
    "print(association_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21afbb18",
   "metadata": {},
   "source": [
    "上述代码使用了一个简单的关联分析示例。首先，准备包含项集的数据集。然后，使用TransactionEncoder将数据集转换为二进制编码矩阵。接下来，使用apriori算法找到频繁项集，可以通过设置min_support参数来控制频繁项集的最小支持度阈值。最后，使用association_rules函数生成关联规则，可以通过设置metric参数和min_threshold参数来控制关联规则的度量指标和最小阈值。\n",
    "\n",
    "运行代码后，您将获得找到的频繁项集和生成的关联规则。频繁项集将显示在一个DataFrame中，包含项集和支持度信息。关联规则也将显示在一个DataFrame中，包含关联规则的前项、后项、支持度、置信度等信息。\n",
    "\n",
    "请注意，上述代码只是一个简单的示例，您可以根据自己的数据集和需求进行相应的调整和优化。关联分析还可以进行更复杂的处理，如设置更多的度量指标、使用更大的数据集等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e4706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
